# Deployments: 2.0

## Motivation

The current 1.5 style of deployment has its own merits and demerits. The demerit being the deployable declaration is very fat. The declarative spec for the deployments needs to be separated from the deployed result (instances - assemblies). In the current model we are tied to OpenNebula (or) any other provider has yielded benefits to get here but the opportunity to change the way we wanted is cumbersome and difficult to manage.

We find that OpenNebula has reliability issues in deletions where it shows vms being deleted but we find ghost vms running.

The edge based management of state of the objects resulted in problems where when we reboot the node, the virtual machines are down with declaration requesting the vms to be up. The edge based management also provided a hit or miss to manage the state of the object which resulted in an inconsistent state of the declaration request.

So what we wanted to follow is accept the fact that failures can happen but we want to be eventually consistent and go level based when the declaration is applied. Which means failure in deployment can happen since certain system isn’t ready but when the system is up, we can go ahead and deploy automatically.

Hence the Rancher style of working is something we would like to follow, that is have our own way to deploy vms, containers and support interface to public cloud players like AWS.

In this design we address just the area of deploying virtual machines, containers, apps, custom apps, services.

* VM, Containers and images are the building blocks for deploying your applications.

* Pods and services allow for containers to communicate with each other and proxy connections.

* Namespace and users provide the space and means for communities to organize and manage their content together.

* Builds allow you to build working images and react to new images.

* Deployments add expanded support for the software development and deployment lifecycle.

* Routes announce your service to the world.

* Templates allow for many objects to be created at once based on customized parameters.

# Requirements

* Deploy & Manage  virtual machines

* Deploy & Manage containers

* Deploy & Manage application from source

* Deploy & Manage apps in a click

* Deploy and connect running apps with each other

# Architecture

![Architecture 2.0](https://github.com/megamsys/verticedev/blob/master/pics/architecture.png)

Refer this [link ](https://docs.google.com/presentation/d/1tzkWbHu6RclA0QWnoEFy9HK0KmISdCjLNfv5QxwJ3Mg/edit?usp=sharing)

# Solution Overview

The architecture for MegamVertice shows the different software components leveraged and extended from openshift origin which is based on kubernetes. This design will focus purely on the above requirements to be met.

The kubernetes provides ability to deploy, manage scale containers. Kubernetes CRI (container runtime interface) provides ability to run virtual machines. The mirantis/virtlet extends CRIProxy with an interface to libvirt. Nodes labelled with **extraRuntime=virtlet** will launch VMs.

Refer *slide 2* in this [link ](https://docs.google.com/presentation/d/1tzkWbHu6RclA0QWnoEFy9HK0KmISdCjLNfv5QxwJ3Mg/edit?usp=sharing)

The deployment will call MegamVertice with the declaration to launch in the region, with the resourceconfig.

We’ll follow the kubernetes model of deployment which will result in pods.

The subsequent section focusses on the preparation needed by the administrator in the nodes and we elaborate more on what happens from the user perspective.

## Prepare Nodes

So during a deployment a cluster of machines are labelled with `region` names in kubernetes.

The nodes which can launch virtual machines will have virtlet running and *label with *

**extraRuntime=virtlet**

**region=chennai**

Refer *slide 3* in this [link](https://docs.google.com/presentation/d/1tzkWbHu6RclA0QWnoEFy9HK0KmISdCjLNfv5QxwJ3Mg/edit?usp=sharing)

Upon setting up kubelet and virtlet in the nodes to run virtual machines, admin using the command line provides all users the ability to deploy vms in the cluster.

The term region and namespace will be used interchangeably.

All the current marketplace items will be loaded by the admin as templates in the namespace **default** available under *examples/megam directory*.

 Admin will set **resourceconfigs** in the namespace **default** by using a command line to load it from *examples/megam/resourceconfig.yaml*.

This is nothing but the flavor in 1.5.x.

**resourceconfigs** merely shows the chunk of **resourcequotas** available for deploying. The current administrative page for flavor can support updating and editing the resource config.

Now that  we have labelled nodes with the **regions**, **resourceconfigs** we’ll start on the deployment perspective from the user front.

## Prepare Addons *optional*

This is optional, but the addons to be added to our cluster will be *[CoreDNS](https://github.com/coredns/coredns)* and *[PrometheusOperator](https://github.com/coreos/prometheus-operator)* that collects the metrics of the running pods using labels.

# User deploying a virtual machine

When the user is logged in, we’ll add an extra step for the user to create their own namespace. This is nothing but `organization` in 1.5.x.

When the user wants to launch a virtual machine, the **`new launch`** panel shows the configured `regions.yaml` and a call is made to **resourceconfigs** to get the cloud **resourcequotas**.

Now there is a missing link here on the management of the available **resourcequotas** for an user. This will be dealt in a *separate billing design*.

When the user selects the **resourceconfig** we display the **templates** from **default** namespace. We assume that we’ll use the *ClusterIp* to allocate a network bridge (default).

The user selects "Ubuntu - 16.04" to deploy, which results in attaching the template “ubuntu_1604” in the users namespace by calling `<users_namespace>/processedtemplates`.

The user selects the option for **SSH**, **Root Password**  or **Stored SSH Pairs** and proceeds to launch.  We formulate a json/yaml with  the following Kinds for a virtual machine.

```yaml
Kind: Secrets  with SSH or  RootPassword
```

````yaml
Kind: DeploymentConfig with

    * nodeSelector = regionname

    * label

        * virtualmachine (equivalent to toscatype in 1.5.x)

    * image=**virtlet**/ubuntu_16.04  (this strictly has to start with **virtlet/**)

    * resourcequotas flattened from resourceconfigs selected

    * restartPolicy

    * livelenessProbe

    * readinessProbe
```

A deployment is pushed by doing as many API calls for the `Kinds` available to MegamVertice. Virtlet receives the request and launches the request.

When the VM is launched `cloud-init` will stick information for SSH and successfully run the VM.

The current strategy for the need to have a `control-manager daemon` inside the virtual machine needs to be studied.

The dashboard will pull the **Pods** and **Deployments** launched under the namespace of the user and display them.
