# Deployments: 2.0

## Motivation

The current 1.5 style of deployment has its own merits and demerits. The demerit being the deployable declaration is very fat. The declarative spec for the deployments needs to be separated from the deployed result (instances - assemblies). In the current model we are tied to OpenNebula (or) any other provider has yielded benefits to get here but the opportunity to change the way we wanted is cumbersome and difficult to manage.

We find that OpenNebula has reliability issues in deletions where it shows vms being deleted but we find ghost vms running.

The `edge based` management of state of the objects resulted in problems where when we reboot the node, the virtual machines are down with declaration requesting the vms to be up. The edge based management also provided a hit or miss to manage the state of the object which resulted in an inconsistent state of the declaration request.

So what we wanted to follow is accept the fact that failures can happen but we want to be eventually consistent and go `level based` when the declaration is applied. Which means failure in deployment can happen since certain system isn’t ready but when the system is up, we can go ahead and deploy automatically.

Hence the Rancher style of working is something we would like to follow, that is have our own way to deploy vms, containers and support interface to OpenNebula, OpenStack, public cloud like Azure, AWS, Softlayer.

In this design we address just the area of deploy, manage virtual machines, containers, apps, custom apps, services.

* VM, Containers and images are the building blocks for deploying your applications.

* Projects and users provide the space and means for communities to organize and manage their content together.

* Deployments add expanded support for the software development and deployment lifecycle.

* Routes announce your service to the world.

# Requirements

* Deploy & Manage  virtual machines

* Deploy & Manage containers

* Deploy & Manage application from source

* Deploy & Manage apps in a click

* Deploy and connect running apps with each other

* Scale, LB vms/containers

* Support for hybrid cloud

* Reduce memory  footprint and ease of using our stack

* CI/CD

# Architecture

![Architecture 2.0](https://github.com/megamsys/verticedev/blob/master/pics/architecture.png)

Refer this [link ](https://docs.google.com/presentation/d/1tzkWbHu6RclA0QWnoEFy9HK0KmISdCjLNfv5QxwJ3Mg/edit?usp=sharing)


# Solution Overview

The architecture for MegamVertice shows the different software components leveraged and extended based on a pull from an API server to react to state changes in object stored in the database.

The deployment will call MegamVertice with the declaration to launch in the region, with the flavor.

## Major changes

### Database

We wanted a better database than `cassandra` as we no longer will store time-series information there.

We picked `cockroachdb`.

###  Standards for declaration

A major change to our architecture is to adhere completely to standard [CAMP 1.2](http://docs.oasis-open.org/camp/camp-spec/v1.2/csprd01/camp-spec-v1.2-csprd01.html)

We toyed around supporting [TOSCA 1.1](http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.1/TOSCA-Simple-Profile-YAML-v1.1.html) fully but this is a nice to have and `adapters for TOSCA` will be supported in the future.

# User deploying a virtual machine

When the user is logged in, we’ll automatically create a unique **project** using the heroku style naming. This is nothing but `organization` in 1.5.x.

When the user wants to launch a virtual machine, the **`new launch`** panel shows the configured `regions.yaml` and a call is made to **flavors** to get the cloud **resourcequotas**.

Now there is a missing link here on the management of the available **resourcequotas** for an user. This is dealt in the  [billing design](05-billing.md)

When the user selects the **flavors** we display the **plans**.

The user selects "Ubuntu - 16.04" to deploy, which results in attaching the plan “ubuntu_1604” in the users namespace by calling `projects/<users_project>/plans`.

The user selects the option for **SSH**, **Root Password**  or **Stored SSH Pairs** and proceeds to launch.  We formulate a json/yaml with  the following Kinds for a virtual machine.


```json
Plan

    * nodeSelector = regionname

    * tag

        * virtualmachine (equivalent to toscatype in 1.5.x)

    * image=**virtlet**/ubuntu_16.04  (this strictly has to start with **virtlet/**)

    * flavors

```

Do we need this ?

```
* restartPolicy

* livelenessProbe

* readinessProbe

```

A deployment is pushed by doing an API calls for the `plans`. There are several daemons that vertice will be broken down to.

- *vert_controller* will start `PlanController`, `AssemblyFactoryController, HorizontalAssemblyScaler, BillerController`

- *vert_scheduler* will scan for `AssemblyFactoryController` and will use the `MultiCloud framework` to launch in the appropriate region.

Every `MultiCloud framework` will load its own configuration. eg:

`opennebula.conf`

The `PlanController` receives the request and creates an `assembly_factory`.

When the VM is launched we follow our usual process and have the `gulp agent` do its job.

Additionally we'll have [prometheus](https://github.com/prometheus/node_exporter) inside the VM that posts metrics and will exposes [using a pull target from prometheus](https://prometheus.io/docs/operating/configuration/) using an automated service discovery(sd) `vertice_sd_config`.

The dashboard will pull the **AssemblyFactor** and **Assembly** launched under the project of the user and display them.
