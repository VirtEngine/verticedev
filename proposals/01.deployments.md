# Deployments: 2.0

## Motivation

The current 1.5 style of deployment has its own merits and demerits. The demerit being the deployable declaration is very fat. The declarative spec for the deployments needs to be separated from the deployed result (instances - assemblies). In the current model we are tied to OpenNebula (or) any other provider has yielded benefits to get here but the opportunity to change the way we wanted is cumbersome and difficult to manage.

We find that OpenNebula has reliability issues in deletions where it shows vms being deleted but we find ghost vms running.

The `edge based` management of state of the objects resulted in problems where when we reboot the node, the virtual machines are down with declaration requesting the vms to be up. The edge based management also provided a hit or miss to manage the state of the object which resulted in an inconsistent state of the declaration request.

So what we wanted to follow is accept the fact that failures can happen but we want to be eventually consistent and go `level based` when the declaration is applied. Which means failure in deployment can happen since certain system isn’t ready but when the system is up, we can go ahead and deploy automatically.

Hence the Rancher style of working is something we would like to follow, that is have our own way to deploy vms, containers and support interface to OpenNebula, OpenStack, public cloud like Azure, AWS, Softlayer.

In this design we address just the area of deploy, manage virtual machines, containers, apps, custom apps, services.

* VM, Containers and images are the building blocks for deploying your applications.

* Projects and users provide the space and means for communities to organize and manage their content together.

* Deployments add expanded support for the software development and deployment lifecycle.

* Routes announce your service to the world.

# Requirements

* Deploy & Manage  virtual machines

* Deploy & Manage containers

* Deploy & Manage application from source

* Deploy & Manage apps in a click

* Deploy and connect running apps with each other

* Scale, LB vms/containers

* Support for hybrid cloud

* Reduce memory  footprint and ease of using our stack

* CI/CD

# Architecture

![Architecture 2.0](https://github.com/megamsys/verticedev/blob/master/pics/architecture.png)

Refer this [link ](https://docs.google.com/presentation/d/1tzkWbHu6RclA0QWnoEFy9HK0KmISdCjLNfv5QxwJ3Mg/edit?usp=sharing)


# Solution Overview

The architecture for MegamVertice shows the different software components leveraged and extended based on a pull from an API server to react to state changes in object stored in the database.

The deployment will call MegamVertice with the declaration to launch in the region, with the flavor.

## Notable changes

### Database

We wanted a better database than `cassandra` as we no longer will store time-series information. Ou intention was to use [ScyllaDB](http://www.scylladb.com) but it didn't support `ALTER TABLE` during **1.5.x**.

We have picked [cockroachdb](https://www.cockroachlabs.com/docs/secure-a-cluster.html). This means we need a migrator, for which we'll write a CLI using `OCaml`.

###  Standards for declaration

A major change to our architecture is to adhere completely to standard [CAMP 1.2](http://docs.oasis-open.org/camp/camp-spec/v1.2/csprd01/camp-spec-v1.2-csprd01.html).

We toyed around supporting [TOSCA 1.1](http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.1/TOSCA-Simple-Profile-YAML-v1.1.html) fully but this is a nice to have and `adapters for TOSCA` will be supported in the future.

[CAMP 1.2](http://docs.oasis-open.org/camp/camp-spec/v1.2/csprd01/camp-spec-v1.2-csprd01.html) is more generalized is catered toward *portability across PaaS players* which has been solved in a different way as well. It doesn't provide all the api needed for us. 

So we'll use the current REST API (eg: backups, snapshots, addons, ..) as needed


### Metrics

We are moving to a pull based metrics collector using [prometheus](https://www.prometheus.io) using the [service discovery approach](https://prometheus.io/docs/operating/configuration/). We  may have to build our own `<vertice_sd_config>` or leveraging the `<remote_read>` to pull metrics for *targets* using the *api* *https://localhost/v3/assembly_factory*. 

Additionally we will have [node_exporters](https://github.com/prometheus/node_exporter) in each of the virtual machines.


# User deploying a virtual machine

## Setup

An admin will setup the default private cloud flavors using the currrent flavors page. Additionally an admin will setup the **cloudsettings** for AWS, Azure, Softlayer, Google, and *OpenNebula - private* with a *region name*, *access_key*, *secret_key*, *user_id*, *password*, *cluster_name*, *endpoint* as applicable.

| Name              | Configured with                   | Description                                                                                                                                            |
|-------------------|-----------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|
| DUBAI_AWS_DEV01   | cloud_type: AWS  region:  ireland | This says we have configured a `cloud_type` of `AWS` with real `aws region` Ireland. This will appear in the launch panel as **DUBAI_AWS_DEV01**       |
| DUBAI_AZURE_DEV01 | cloud_type: AZURE region: Ireland | This says we have configured a `cloud_type` of `AZURE` with real `azure region` Ireland. This will appear in the launch panel as **DUBAI_AZURE_DEV01** |
| DUBAI_DEFAULT     | cloud_type: ONE region: Dubai_01  | This says we have configured a `cloud_type` of `ONE` with `one cluster` Dubai01. This will appear in the launch panel as **DUBAI_AZURE_DEV01**         |
| Chennai           | cloud_type: ONE                   | This says we have configured a `cloud_type` of `ONE` with `one cluster` Chennai. This will appear in the launch panel as **Chennai**                   |

## Deploying

When the user is logged in, we’ll automatically create an unique **project** using the `heroku style` naming. This is nothing but `organization` in 1.5.x.

When the user wants to launch a virtual machine, the **new launch** panel shows the configured `regions` and a call is made to **flavors** to get the cloud **flavors**.

Now there is a missing link here on the management of the available **resourcequotas** for an user. This is dealt in the  [billing design](05-billing.md)

When the user selects the **flavor** we display the **services** in **plans**.

The user selects the option for **SSH**, **Root Password**  or **Stored SSH Pairs** and proceeds to launch.  

We formulate a `components` as per our need and run several REST cals. Upon completion we call `assembly` with the `components` url pointer. 

There are several daemons that will handle our stuff.

- *magudi* will start `PlanController`, `DeploymentController, HorizontalAssemblyScaler, BillerController`

- *DeploymentController* will scan for `assembly resource` to check its status to see if its running, or just created and will set the "flag" to "SCHEDULE_".

- *beedi* will scan for `assembly resource` to check its status to see if it needs to "SCHEDULE_" and will launch or manage using its `MultiCloud framework` in the appropriate region.

The cloudsettings will be configured in our admin UI in nilavu. So the`MultiCloud framework` will pull that information with regards to the settings and launch.

When the VM is launched we follow our usual process and have the `gulp agent` do its job.

Additionally we'll have [prometheus](https://github.com/prometheus/node_exporter) inside the VM that posts metrics and will exposes [using a pull target from prometheus](https://prometheus.io/docs/operating/configuration/) using an automated service discovery(sd) `vertice_sd_config`.

The dashboard will pull the **assembly_factory** and **assembly** launched under the project of the user and display them.


## /v3/plan_factory

plan_factory returns all the builtin plans stores (like the *1.5.x*  *markplaces*)

### /v3/plan/ubuntu machine

We use `services` in CAMP to say that a `14.04 image` is needed to run the *“name”: “Ubuntu”*

For multiple versions of the same image, add a another service.

So we say in our current 1.5 *plans in marketplaces = services*

We’ll name using the naming format format *<order>_<cattype>_<name>*

```json
{
   "name":"1_virtualmachine_ubuntu",
   "description":"Ubuntu is a Debian-based Linux operating system.",
   "tags":"[linux, ubuntu, xenial, 14.04]",
   "camp_version":"1.2",
   "origin":"vertice:2.0",
   "artifacts":[ ],
   "services":[{
         "name":"Trusty",
         "description":"Ubuntu is a Debian-based Linux operating system. Trusty Tahr is the Ubuntu codename for version 14.04 LTS of the Ubuntu Linux-based operating system.",
         "href":"https://www.ubuntu.com",
         "Characteristics":[
            {
               "type":"org.megam.vm::provided_by",
               "14.04":"vertice"
            }
         ]
      }
   ]
}
```

### /v3/plan/java

```
{
   "name":"2_application_java",
   "description":"The Apache Tomcat® software is an open source implementation of the Java Servlet, JavaServer Pages, Java Expression Language and Java WebSocket technologies.",
   "tags":[
      "tomcat",
      "java",
      "jdk"
   ],
   "camp_version":"1.2",
   "origin":"vertice:2.0",
   "artifacts":[
     ],
   "services":[
      {
         "name":"tomcat",
         "description":"",
         "href":"http://tomcat.apache.org/",
         "characteristics":[
           {
               "type":"org.megam.java::inputs",
               "os":"centos"
            },
            {
               "type":"org.megam.java::env",
               "http.port":"8080"
            },
            {
                "type":"org.megam.java:login",
                "megam":"team4megam"
            }
          {
               "type":"org.megam.java::provided_by",
               "1.8":"vertice"
            },
            {
               "type":"org.megam.java::display_image",
               "image":"java.png"
            },
            {
               "(or) type":"org.megam.java::display_image_url",
               "url":"https://testing/java.png"
            },
         ]
      }
   ]
}
```

### /v3/plan/apache

```json
  {
   "name":"3_webserver_apache",
   "description":"Apache is the most widely used web server software",
   "tags":"[apache, httpd]",
   "camp_version":"1.2",
   "origin":"vertice:2.0",
   "artifacts":[

   ],
   "services":[
      {
         "name":"Apache",
         "description":"Apache is the most widely used web server software. Developed and maintained by Apache Software Foundation",
         "href":"https://www.apache.org",
         "characteristics":[
            {
               "type":"org.megam.webserver::provided_by",
               "2.4.18":"vertice"
            },
            {
               "type":"org.megam.webserver::display_image",
               "image":"apache.png"
            },
            {
               "(or) type":"org.megam.webserver::display_image_url",
               "url":"https://testing/apache.png"
            },
            {
               "type":"org.megam.webserver::inputs",
               "os":"centos"
            },
            {
               "type":"org.megam.webserver::env",
               "http.port":"80"
            }
         ]
      }
   ]
}
```

### /v3/assembly_factory?sort=+name

So we say in our current 1.5 **assemblies = assembly_factory**

```json
{  
   "uri":"/assembly_factory",
   "name":"assembly_factory",
   "tags":[ "asm001",   "asm002",  "asm003"  ],
   "description":"Collection of Assembly",
   "representation_skew": UNKNOWN ,
   "total_items":3,
   "Items_per_page":3,
   "start_index":0,
   "items":[  
      {  
         "uri":"/v3/assembly/asm001",
           "name":"asm001",
    "status":"RUNNING"
      },
      {  
         "uri":"/v3/assembly/asm002",
         "name":"asm002",
           "status":"PAUSED"
             },
      {  
         "uri":"/v3/assembly/asm003",
         "name":"asm003",
         "status":"RUNNING"
             }
   ],
  }
```

### /v3/assembly/ASM0001

We want to track the dynamic elements of a running application [virtualmachines, collaboration, customapp]. This is termed as [components in CAMP](http://docs.oasis-open.org/camp/camp-spec/v1.2/csprd01/camp-spec-v1.2-csprd01.html#_Toc482874847).


| tag                  | name                       | Description                                                                                                     |
|----------------------|----------------------------|-----------------------------------------------------------------------------------------------------------------|
| flavor               | DeployedFlavor             | tell us the details of the flavour (cpu, ram, disk)                                                             |
| network_public_ipv4  | NetworkAttachPublicIPV4To  | tell us the details of the private ipv4 attached                                                                |
| network_private_ipv4 | NetworkAttachPrivateIPV4To | tell us the details of public ipv4                                                                              |
| region               | ClusterIAMIn               | tell us the cluster or grouping i am part of                                                                    |
| security             | Identity                   | tell us the details of ssh or userpassw                                                                         |
| appsecurity          | AppSecurity                | tell us the details of initial application credentials,for logging in                                           |
| appaccess            | AppAccess                  | tell us the details of accessing an application. This should be an URL based on routes resource                 |
| apprepository        | AppRepository              | tells us the deployed url of the repository and its type (svn, git ..)                                          |
| databaseinstance     | DatabaseOn                 | tells us the deployed database details (userid, password and url to access the db like psql://192.178..@root:pw |
|


```json
{
   "uri":"/assembly/ASM0001",
   "name":"testing.megambox.com-90001",
   "description":"ubuntu  installation",
   "tags":[
      "ubuntu"
   ],
   "representation_skew":"NONE",
   "external_management_resource":"",
   "component_collection":[
      "/comps/flav0001",
      "/comps/dbinstance0001"
   ],
   "plan":"/plan/ubuntu",
   "operation_collection":[

   ],
   "sensor_collection":[

   ],
   "metadata":{
      "type_definition":"http://example.org/my_paas/types/assembly",
      "mutable":[
         "/name",
         "/description"
      ],
      "consumer_mutable":[
         "/description"
      ]
   }
}
```
